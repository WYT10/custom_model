{
camera setup -->
    from maix import camera

set frame size -->
    camera.camera.config(size=(240,320))

get frame -->
    def lcdRotation(inputImg):

        imageRotationBuffer = inputImg.crop(0, 0, 240, 320)
        if ScreenOrientation:
            imgRotationAim = image.new(size = (240, 320))
            rotationAngle = +180
        else:
            imgRotationAim = image.new(size = (320, 240))
            rotationAngle = +90
        return imgRotationAim.draw_image(imageRotationBuffer.rotate(rotationAngle, adjust=1),0,0,alpha=1)



    frame = lcdRotation(camera.capture())
}

{
press A/B/C/D button -->
    import sys
    sys.path.append("/root/")
    from CocoPi import BUTTON

    key_A = BUTTON(14)
    key_B = BUTTON(8)
    key_C = BUTTON(13)
    key_D = BUTTON(7)



    if (key_X.is_pressed()):
        ...

release A/B/C/D -->
    import sys
    sys.path.append("/root/")
    from CocoPi import BUTTON

    key_A = BUTTON(14)
    key_B = BUTTON(8)
    key_C = BUTTON(13)
    key_D = BUTTON(7)



    if (key_X.is_pressed() == False):
        ...
}

{
get light sensor -->
    class v83x_ADC():
        def __init__(self, addr=b"0x05070080") -> None:
            self.addr = addr
            self.path = "/sys/class/sunxi_dump/dump"
            self.file = open(self.path, "wb+")
            self.last = self.value()
        def __del__(self):
            try:
                if self.file:
                    self.file.close()
                    del self.file
            except Exception as e:
                pass
        def value(self):
            self.file.write(b"0x05070080")
            self.file.seek(0)
            return int(self.file.read()[:-1], 16)

    v831_adc0 = v83x_ADC()

    light_d = v831_adc0.value()

get temp sensor -->
    import sys
    sys.path.append("/root/")
    from CocoPi import AHT20

    aht20 = AHT20(2)

    temp = aht20.get_temperature()
}

{
LCD setup -->
    from maix import display
    from maix import image
    from maix import camera

    camera.camera.config(size=(240,320))
    image.load_freetype("/root/preset/fonts/simhei.ttf")

    ScreenOrientation = False #True: Portrait; False: Landscape

set front -->
    image.load_freetype("/root/preset/fonts/CascadiaCodePL-Italic.ttf")

draw blank canvas -->
    canvas = image.new(size = (320, 240)) #width, height

fill color -->
    if ScreenOrientation:
        canvas = image.new(size = (240, 320), color = (0,0,0), mode = "RGB")
    else:
        canvas = image.new(size = (320, 240), color = (0,0,0), mode = "RGB")
    # rgb

update frame -->
    if ScreenOrientation:
        canvasVER = canvas.crop(0,0,240,320)
        canvasVER = canvasVER.rotate(-90, adjust=1)
        display.show(canvasVER)
    else:
        display.show(canvas)

clear frame -->
    canvas.clear()

rotate frame -->
    def screenShow(inputImg,rotationAngle):
        #global img_drop

        if rotationAngle==90 or rotationAngle == 270:
            screenCapture=inputImg.crop(0,0,240,320)
        else:
            screenCapture=inputImg.crop(0,0,320,240)
        canvas_screenShow = screenCapture.rotate(-rotationAngle,adjust=1)
        display.show(canvas_screenShow)



    # screenShow(canvas,0)

put txt -->
    canvas.draw_string(0,0, "", scale = 1, color = (255,0,0) , thickness = 1)

draw line -->
    canvas.draw_line(0,0, 0,0, color=(255,0,0), thickness=1)

draw retangle -->
    canvas.draw_rectangle(0,0,0,0, color=(255,0,0), thickness=-1)

darw circle -->
    canvas.draw_circle(0,0, 2, color=(255, 0, 0), thickness=-1)

add image -->
    canvas.draw_image((image.open("/root/user/img/saved.jpg")),0,0,alpha=1)
}

{
set up mic -->
    import pyaudio
    import wave
    import os

set path & duration -->
    if(os.path.exists("/root/user/audio/record.wav")): # <--
        os.system("rm /root/user/audio/record.wav") # <--
    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    RECORD_SECONDS = 5 # <--
    WAVE_OUTPUT_FILENAME = "/root/user/audio/record.wav" # <--

record -->
    p = pyaudio.PyAudio()

    stream = p.open(format=FORMAT,channels=CHANNELS,rate=RATE,input=True,frames_per_buffer=CHUNK)

    print("* recording")

    frames = []

    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
        data = stream.read(CHUNK,exception_on_overflow = False)
        frames.append(data)

    print("* done recording")

    stream.stop_stream()
    stream.close()
    p.terminate()

    wf = wave.open(WAVE_OUTPUT_FILENAME, "wb")
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b"".join(frames))
    wf.close()
}

{
set up speaker -->
    import os
    import pyaudio
    import wave
    import time
    import sys
    sys.path.append("/root/")

set volume -->
    def voice_numberMap(value):
        valueScaled = float(value - 0) / float(100)
        return int(valueScaled * 31)

    CHUNK = 1024
    VOICESTATE = 0

    VOICENUMP = str(voice_numberMap(25)) # <--
    time.sleep(0.01)
    SYSTEMVOICE = "amixer cset numid=8,iface=MIXER,name=\"LINEOUT volume\" "+ VOICENUMP+""

laod file -->
    if VOICESTATE == 0:
        wf = wave.open("/root/preset/audio/luckystar.wav", "rb") # <--
        p = pyaudio.PyAudio()

        stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),channels=wf.getnchannels(),rate=wf.getframerate(),output=True)

        data = wf.readframes(CHUNK)
        VOICESTATE = 1

play audio -->
    if VOICESTATE == 1:
        if len(data) > 0:
            try:
                stream.write(data)
                data = wf.readframes(CHUNK)
            except:
                VOICESTATE = 0
        else:
            stream.stop_stream()
            stream.close()
            p.terminate()
            VOICESTATE = 0

end play -->
    stream.stop_stream()
    stream.close()
    p.terminate()
}

{
turn off -->
    os.system('poweroff')

reboot -->
    os.system('reboot')

refresh dict -->
    os.system('sync')

system voice -->
    def voice_numberMap(value):
        valueScaled = float(value - 0) / float(100)
        return int(valueScaled * 31)

    VOICENUMP = str(voice_numberMap(25)) # <--
    time.sleep(0.01)
    SYSTEMVOICE = "amixer cset numid=8,iface=MIXER,name=\"LINEOUT volume\" "+ VOICENUMP+""
    os.system(SYSTEMVOICE)

** threading -->
    import threading

    def printNumber(n: int) -> None:
        while True:
            thread_calsss_fun()

    def thread_calsss_fun():
        global _BME,canvas,img_graphTransmission,ScreenOrientation,CHUNK,VOICESTATE



    CocoPiThread = threading.Thread(target=printNumber, args=(1,))
    CocoPiThread.start()
}

{
save canvca -->
    canvas.save("/root/user/img/saved.jpg")

open image -->
    x = image.open("/root/user/img/saved.jpg")

flip image -->
    x = canvas.flip(0) #0: vertical; 1: horizontal

rotate image -->
    x = canvas.rotate(-50, adjust=1) #-:clockwise; +:anti-clockwise

resize image -->
    x = canvas.resize(0, 0, padding = 0) # (0, 0, ...)

crop image -->
    x = canvas.crop(1, 0,3, 0) #x, y, width, height

default color -->
    Black: [(0,40)]
    white: [(64,100)]
    light red: [(45, 65, 40, 80, 40, 60)]
    general green: [(45, 65, -50, -30, 0, 40)]
    light blue: [(45, 65, -20, 30, -60, -20)]
    orange: [(77, 55, 19, 61, 14, 108)]

lane detection -->
    line = canvas.find_line() #detection
    
    #draw lines <-- color
    canvas.draw_line(line["rect"][0], line["rect"][1], line["rect"][2],line["rect"][3], color=(0,0,0), thickness=1) 
    canvas.draw_line(line["rect"][2], line["rect"][3], line["rect"][4],line["rect"][5], color=(0,0,0), thickness=1)
    canvas.draw_line(line["rect"][4], line["rect"][5], line["rect"][6],line["rect"][7], color=(0,0,0), thickness=1)
    canvas.draw_line(line["rect"][6], line["rect"][7], line["rect"][0],line["rect"][1], color=(0,0,0), thickness=1)
    
    #plot the center <-- color
    canvas.draw_circle(line["cx"], line["cy"], 4,color=(0,0,0), thickness=1)

color range detection-->
    def v831_find_blob_fun(lane_trackingimg,region1,COLOR):
        IMGBLOB = lane_trackingimg.crop(region1[0],region1[1],region1[2],region1[3])
        blobsCANVAS = IMGBLOB.find_blobs(COLOR, merge=True)
        return blobsCANVAS

    x = v831_find_blob_fun(canvas,(0, 0,0, 0),([(0,40)])) # (canvas, (x,y, width, height), (default colors))

color line detection -->
    _ld_color_detection_threshold = ([(0,40)]) # --> default colors
    def v831_lane_tracking_setup_one(lane_trackingimg,region1):
        global lane_trackingline
        lane_trackingcanvas = lane_trackingimg.crop(region1[0],region1[1],region1[2],region1[3])
        lane_trackingline = lane_trackingcanvas.find_line()
        lane_trackingcanvas.draw_circle(lane_trackingline["cx"], lane_trackingline["cy"], 2,color=(0,255,0), thickness=1)
        lane_trackingimg.draw_image(lane_trackingcanvas,region1[0],region1[1])
        lane_trackingimg.draw_rectangle(region1[0],region1[1],region1[0]+region1[2],region1[1]+region1[3], color=(255,0,0), thickness=1)
        lane_trackingline["cx"] = lane_trackingline["cx"] + region1[0]
        lane_trackingline["cy"] = lane_trackingline["cy"] + region1[1]
        return lane_trackingline

    lane_trackingline1 = {}



    x = v831_lane_tracking_setup_one(canvas,(0, 0,0, 0)) #(canvas, (x,y, width, height))

2 region color lane detection -->
    _ld_color_detection_threshold = ([(0,40)])
    def v831_lane_tracking_setup(lane_trackingimg,region1,region2):
        global lane_trackingline1,lane_trackingline2
        lane_trackingcanvas1 = lane_trackingimg.crop(region1[0],region1[1],region1[2],region1[3])
        lane_trackingcanvas2 = lane_trackingimg.crop(region2[0],region2[1],region2[2],region2[3])
        lane_trackingline1 = lane_trackingcanvas1.find_line()
        lane_trackingline2 = lane_trackingcanvas2.find_line()
        lane_trackingcanvas1.draw_circle(lane_trackingline1["cx"], lane_trackingline1["cy"], 2,color=(0,255,0), thickness=1)
        lane_trackingcanvas2.draw_circle(lane_trackingline2["cx"], lane_trackingline2["cy"], 2,color=(0,255,0), thickness=1)
        lane_trackingimg.draw_image(lane_trackingcanvas1,region1[0],region1[1])
        lane_trackingimg.draw_image(lane_trackingcanvas2,region2[0],region2[1])
        lane_trackingimg.draw_rectangle(region1[0],region1[1],region1[0]+region1[2],region1[1]+region1[3], color=(255,0,0), thickness=1)
        lane_trackingimg.draw_rectangle(region2[0],region2[1],region2[0]+region2[2],region2[1]+region2[3], color=(255,0,0), thickness=1)

    lane_trackingline1 = {}
    lane_trackingline2 = {}

    v831_lane_tracking_setup(canvas,(0, 140,100, 240),(220, 140,320, 240)) # (canvas, (x,y, width, height), (x,y, width, height))

get color from region -->
    canvas.get_blob_color((0, 0, 0, 0), 0, 0) # ((x,y, width, height), ..)

qr-code -->
    x = canvas.find_qrcodes() #scan
    data = i["payload"] # decode data / x,y,w,h
}

{
video ini -->
    import os
    import pyaudio
    import av
    from maix import display
    from maix import camera
    from maix import image

load video -->
    pathToVideo = "/root/user/video/record.mp4"
    containerVideo = av.open(pathToVideo)
    streamVideo = containerVideo.streams.video[0]

play -->
    if "Video" in repr(i):
        VideoImg = image.load(bytes(i.to_rgb().planes[0]), (streamVideo.width, streamVideo.height))
        display.show(VideoImg) #break to pause

video prop. -->
    containerVideo.decode(video=0)

video record ini -->
    import os
    import pyaudio
    import av
    from maix import display
    from maix import camera
    from maix import image

    path_to_video = "/root/user/video/record.mp4"
    fps = 10
    container = av.open(path_to_video, mode='w')
    stream = container.add_stream('h264', rate=fps) # h264 or mpeg4
    stream.width = 320
    stream.height = 240
    stream.pix_fmt = 'yuv420p'

record -->
    frame = av.VideoFrame(canvas.width, canvas.height, 'rgb24')
    frame.planes[0].update(canvas.tobytes())
    for packet in stream.encode(frame):
        container.mux(packet)

stop -->
    for packet in stream.encode():
        container.mux(packet)

    container.close()
}

{
edge detection ini -->
    import numpy as np

    class Edge:
        model = {
            "param": "/root/preset/model/sobel_int8.param",
            "bin": "/root/preset/model/sobel_int8.bin"
        }
        input_size = (224, 224, 3)
        output_size = (222, 222, 3)
        options = {
            "model_type":  "awnn",
            "inputs": {
                "input0": input_size
            },
            "outputs": {
                "output0": output_size
            },
            "mean": [127.5, 127.5, 127.5],
            "norm": [0.0078125, 0.0078125, 0.0078125],
        }
        def __init__(self):
            from maix import nn
            print("-- load model:", self.model)
            self.model = nn.load(self.model, opt=self.options)
            print("-- load ok")
        def __del__(self):
            del self.model

    m = Edge()

load % edge detect -->
    out = m.model.forward(img_edgedetection, quantize=True, layout="hwc") # img_edgedetection (frame)
    out = out.astype(np.float32).reshape(m.output_size)
    out = (np.ndarray.__abs__(out) * 255 / out.max()).astype(np.uint8)
    data = out.tobytes()
    edgeModel = img_edgedetection.load(data,(222, 222), mode="RGB")

result -->
    edgeModel

handwrite ini -->
    from maix import nn
    from maix.nn import decoder

    class Number_recognition:
        mdsc_path = "/root/preset/model/Number.mud"
        labels = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
        anchors = [1.0, 5.0, 1.35, 5.42, 0.49, 2.55, 0.86, 3.75, 0.65, 4.38]

        def __init__(self):
            self.model = nn.load(self.mdsc_path)
            self.decoder = decoder.Yolo2(len(self.labels) , self.anchors , net_in_size = (224, 224) ,net_out_size = (7,7))
        def __del__(self):
            del self.model
            del self.decoder
        def cal_fps(self ,start , end):
            one_second = 1
            one_flash = end - start
            fps = one_second / one_flash
            return  fps

    number_recognition = Number_recognition()

load % handwrite detect -->
    out = number_recognition.model.forward(img_mnist, quantize=1, layout = "hwc")
    boxes, probs = number_recognition.decoder.run(out, nms=0.5, threshold=0.3, img_size=(224,224))
    if len(boxes):
        for boxesi, box in enumerate(boxes):
            boxes[boxesi].append(probs[boxesi])

results -->
    recognized number: number_recognition.labels[i[4][0]] or len(boxes)
    conf: i[4][1][i[4][0]]*100
    x,y,w,h: 0,1,2,3
    cx, cy: int((i[0] +i[0] + i[2])/2), int((i[1] + i[1] + i[3])/2)

obj detect ini -->
    from maix import nn
    from maix.nn import decoder

    model = {
        "param": "/root/preset/model/yolo2_20class_awnn.param",
        "bin": "/root/preset/model/yolo2_20class_awnn.bin"
    }
    options = {
        "model_type":  "awnn",
        "inputs": {
            "input0": (224, 224, 3)
        },
        "outputs": {
            "output0": (7, 7, (1+4+20)*5)
        },
        "mean": [127.5, 127.5, 127.5],
        "norm": [0.0078125, 0.0078125, 0.0078125],
    }
    labels = ["aeroplane","bicycle","bird","boat","bottle","bus","car","cat","chair","cow","diningtable","dog","horse","motorbike","person","pottedplant","sheep","sofa","train","tvmonitor"]
    anchors = [5.4, 5.38, 1.65, 2.09, 0.8, 1.83, 2.45, 4.14, 0.46, 0.8]
    m = nn.load(model, opt=options)
    yolo2_decoder = decoder.Yolo2(len(labels), anchors, net_in_size=(options["inputs"]["input0"][0], options["inputs"]["input0"][1]), net_out_size=(7, 7))

load % obj detect -->
    out = m.forward(img_objectrecognition.tobytes(), quantize=True, layout="hwc")
    boxes, probs = yolo2_decoder.run(out, nms=0.3, threshold=0.3, img_size=(options["inputs"]["input0"][0], options["inputs"]["input0"][1]))
    if len(boxes):
        for boxesi, box in enumerate(boxes):
            boxes[boxesi].append(probs[boxesi])

results -->
    name: labels[i[4][0]]
    conf: i[4][1][i[4][0]]*100
    x,y,w,h: 0,1,2,3
    cx, cy: int((i[0] +i[0] + i[2])/2), int((i[1] + i[1] + i[3])/2)

guesswork ini -->
    from maix import nn
    from maix.nn import decoder

    class Mora:
        mud_path = "/root/preset/model/mora_int8.mud"
        labels = ["Scissors", "Stone" ,"Paper"]
        anchors = [3.23, 3.25, 1.47, 1.55, 5.09, 5.33, 4.03, 4.28, 2.12, 2.56]

        def __init__(self) -> None:
            from maix import nn
            self.model = nn.load(self.mud_path)
            from maix.nn import decoder
            self.decoder = decoder.Yolo2(len(self.labels) , self.anchors , net_in_size = (224, 224) ,net_out_size = (7,7))

        def __del__(self):
            del self.model
            del self.decoder

        def cal_fps(self ,start , end):
            one_second = 1
            one_flash = end - start
            fps = one_second / one_flash
            return  fps

        def draw_rectangle_with_title(self ,img, box, disp_str , fps ):
            img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3],color=(255, 0, 0), thickness=2)
            img.draw_string(box[0], box[1] ,disp_str, scale=1,color=(222, 0, 3), thickness=2)
            #img.draw_string(0, 0 ,"FPS :"+str(fps), scale=2 ,color=(0, 0, 255), thickness=2)

        def process(self,input):
            t =  time()
            out = self.model.forward(input, quantize=1, layout = "hwc")
            boxes, probs = self.decoder.run(out, nms=0.5, threshold=0.5, img_size=(224,224))
            for i, box in enumerate(boxes):
                class_id = probs[i][0]
                prob = probs[i][1][class_id]
                disp_str = "{}:{:.2f}%".format(self.labels[class_id], prob*100)
                fps = self.cal_fps(t, time())
                self.draw_rectangle_with_title(input, box, disp_str, fps)

    Mora = Mora()

load % guesswork detect -->
    out = Mora.model.forward(img_guessworkrecognition, quantize=1, layout = "hwc")
    boxes, probs = Mora.decoder.run(out, nms=0.5, threshold=0.5, img_size=(224,224))
    if len(boxes):
        for boxesi, box in enumerate(boxes):
            boxes[boxesi].append(probs[boxesi])

results -->
    name: labels[i[4][0]]
    conf: i[4][1][i[4][0]]*100
    x,y,w,h: 0,1,2,3
    cx, cy: int((i[0] +i[0] + i[2])/2), int((i[1] + i[1] + i[3])/2)

face detection ini -->
    from maix import nn
    from maix.nn import decoder

    model = {
        "param": "/root/preset/model/yolo2_face_awnn.param",
        "bin": "/root/preset/model/yolo2_face_awnn.bin"
    }
    labels = ["person"]
    options = {
        "model_type":  "awnn",
        "inputs": {
            "input0": (224, 224, 3)
        },
        "outputs": {
            "output0": (7, 7, (1+4+len(labels))*5)
        },
        "mean": [127.5, 127.5, 127.5],
        "norm": [0.0078125, 0.0078125, 0.0078125],
    }
    anchors = [1.19, 1.98, 2.79, 4.59, 4.53, 8.92, 8.06, 5.29, 10.32, 10.65]
    m = nn.load(model, opt=options)
    yolo2_decoder = decoder.Yolo2(len(labels), anchors, net_in_size=(options["inputs"]["input0"][0], options["inputs"]["input0"][1]), net_out_size=(7, 7))

load % face detect -->
    out = m.forward(img_facedetection.tobytes(), quantize=True, layout="hwc")
    boxes, probs = yolo2_decoder.run(out, nms=0.3, threshold=0.3, img_size=(options["inputs"]["input0"][0], options["inputs"]["input0"][1]))
    if len(boxes):
        for boxesi, box in enumerate(boxes):
            boxes[boxesi].append(probs[boxesi])

results -->
    name: 'face'
    conf: i[4][1][i[4][0]]*100
    x,y,w,h: 0,1,2,3
    cx, cy: int((i[0] +i[0] + i[2])/2), int((i[1] + i[1] + i[3])/2)

license detect ini -->
    from maix import nn
    from maix.nn import decoder

    class LPR:
        loc_model_path = "/root/preset/model/loc.mud"
        reg_model_path = "/root/preset/model/reg.mud"
        chars = ["皖", "沪", "津", "渝", "冀", "晋", "蒙", "辽", "吉", "黑",
                "苏", "浙", "京", "闽", "赣", "鲁", "豫", "鄂", "湘", "粤",
                "桂", "琼", "川", "贵", "云", "藏", "陕", "甘", "青", "宁",
                "新", "警", "学", "A", "B",  "C",  "D",  "E",  "F",  "G",
                "H",   "J",  "K",  "L", "M", "N",  "P",  "Q",  "R", "S",
                        "T",  "U",  "V", "W", "X", "Y", "Z", "0", "1", "2", "3",
                        "4", "5", "6", "7", "8", "9", "-"]

        variances = [0.1, 0.2]
        steps = [8, 16, 32]
        min_sizes = [12, 24, 48, 96, 192, 320]
        card_input_img = None

        def __init__(self) -> None:
            from maix import nn
            self.loc_model = nn.load(self.loc_model_path, opt=None)
            self.reg_model = nn.load(self.reg_model_path, opt=None)

            from maix.nn import decoder
            self.loc_decoder = decoder.license_plate_location(
                [224, 224], self.steps, self.min_sizes, self.variances)
            self.reg_decoder = decoder.CTC((1, 68, 18))

        def __del__(self):
            del self.loc_model
            del self.loc_decoder

        def cal_fps(self, start, end):
            one_second = 1
            one_flash = end - start
            fps = one_second / one_flash
            return fps

        def draw_fps(self, img, fps):
            img.draw_string(0, 0, "FPS :"+str(fps), scale=1,
                            color=(255, 0, 255), thickness=1)

        def draw_string(self, img, x, y, string, color):
            img.draw_string(x, y, string, color=color)

        def draw_paste(self, src, dst):
            src.paste(dst, 0, 0)

        def draw_rectangle(self, img, box):
            img.draw_rectangle(box[0], box[1], box[2], box[3],
                            color=(230, 230, 250), thickness=2)

        def draw_point(self, img, landmark):
            for i in range(4):
                x = landmark[2 * i]
                y = landmark[2 * i + 1]
                img.draw_rectangle(x-2, y-2, x+2, y+2,
                                color=(193, 255, 193), thickness=-1)

        def process(self, input):
            global CARDDATABOEX
            self.card_input_img = input
            # retinaface decoder only support chw layout
            loc_out = self.loc_model.forward(input, quantize=1, layout="chw")
            boxes, landmarks = self.loc_decoder.run(loc_out, nms=0.2, score_thresh=0.7, outputs_shape=[
                                                    [1, 4, 2058], [1, 2, 2058], [1, 8, 2058]])
            if len(boxes):
                # print(boxes,landmarks)
                for boxesi, box in enumerate(boxes):
                    boxes[boxesi].append(landmarks[boxesi])
            CARDDATABOEX = boxes
        def get_card_data(self, landmark):
            # landmark = i[4][:6]
            reg_in = self.card_input_img.crop_affine(landmark, 94, 24)
            reg_out = self.reg_model.forward(reg_in,  quantize=1, layout="chw")

            LP_number = self.reg_decoder.run(reg_out)
            string_LP = ""
            for id in LP_number:
                string_LP += self.chars[id]
            return string_LP

    LPRCARD  = LPR()

    CARDDATABOEX = ""

load % license detect -->
    LPRCARD.process(canvas)

results -->
    name: LPRCARD.get_card_data(i[4][:6])
    conf: i[4][1][i[4][0]]*100
    x,y,w,h: 0,1,2-0,3-1
    cx, cy: int((i[0] + i[2])/2), int((i[1] + i[3])/2)

face recognition ini -->
    from maix import nn
    from maix.nn.app import face
    from maix.nn.app.face import FaceRecognize

    max_face_num = 4
    detect_threshold = 0.5
    detect_nms = 0.3
    #score_threshold = 70
    FEATURES = []

    score_threshold = 70

    CLASSNAMEFACELIST = ["Name1", "Name2", "Name3"]

load % face recognize detect -->
    class Face_Recognizer:
        def __init__(self, threshold = 0.5, nms = 0.3, max_face_num = 1):
            model = "/root/preset/model/retinaface.mud"
            model_fe = "/root/preset/model/fe_resnet.mud"
            self.input_size = (224, 224, 3)
            input_size_fe = (128, 128, 3)
            self.feature_len = 256
            self.features = []
            print("-- load model:", model)
            m = nn.load(model)
            print("-- load ok")
            print("-- load model:", model_fe)
            m_fe = nn.load(model_fe)
            print("-- load ok")

            self.recognizer = FaceRecognize(m, m_fe, self.feature_len, self.input_size, threshold, nms, max_face_num)
            print("-- init end")

        def get_faces(self, img, std_img = False):
            faces = self.recognizer.get_faces(img, std_img)
            return faces

        def __len__(self):
            return len(self.features)

        def add_user(self, name, feature):
            self.features.append([name, feature])
            return True

        def remove_user(self, name_del):
            rm = None
            for name, feature in self.features:
                if name_del == name:
                    rm = [name, feature]
            if rm:
                self.features.remove(rm)
                return True
            return False

        def recognize(self, feature):
            max_score = 0
            uid = -1
            for i, user in enumerate(self.features):
                score = self.recognizer.compare(user[1], feature)
                if score > max_score:
                    max_score = score
                    uid = i
            if uid >= 0:
                return self.features[uid][0], max_score
            return None, 0

        def get_input_size(self):
            return self.input_size

        def get_feature_len(self):
            return self.feature_len

        def darw_info(self, img, box, points, disp_str, bg_color=(255, 0, 0, 255), font_color=(255, 255, 255, 255), font_size=32):
            font_wh = image.get_string_size(disp_str)
            for p in points:
                img.draw_rectangle(p[0] - 1, p[1] -1, p[0] + 1, p[1] + 1, color=bg_color)
            img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)
            if disp_str:
                img.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)
                img.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)

        def darw_title(self, img, dis_size ,key_l = None, key_r =None):
            if key_C:
                key_l = "| "+ key_l
                img.draw_string( 1, 2 ,key_l , scale = 1, color = (255, 255, 255), thickness = 2)
            if key_D:
                key_r = key_r+" |"
                w = int(dis_size[0] - 4 - image.get_string_size(key_r)[0] * 1)
                img.draw_string( w, 2 ,key_r , scale = 1, color = (255, 255, 255), thickness = 2)

    FACERECGNIZER = Face_Recognizer(detect_threshold, detect_nms, max_face_num = max_face_num)

    FACESRECOGNITONRESULT = FACERECGNIZER.get_faces(img_face_Recognizer)

results -->
    face info: i[3]
    conf: i[0]
    x,y,w,h: i[1][0], i[1][1], i[1][2], i[1][3]
    cx, cy: int((i[1][2]+i[1][0]+i[1][0])/2), int((i[1][3]+i[1][1]+i[1][1])/2)

recognize any face result -->
    len(FACESRECOGNITONRESULT)

save face data -->
    FACERECGNIZER.add_user(CLASSNAMEFACELIST[len(FACERECGNIZER)], save_face_data) # save face data
    FEATURES = FACERECGNIZER.features

del last saved face data -->
    FACERECGNIZER.remove_user(CLASSNAMEFACELIST[len(FACERECGNIZER)-1])
    FEATURES = FACERECGNIZER.features

save face data to path -->
    import os
    import json

    def _CREATE_TEXT_FILE_WITH_CONTENT(_path, _data, _sep):
        f = open(_path, "a")
        f.write(_data + _sep)
        f.close()

    try:
        os.remove("/root/user/model/recorded_face_features.py")
    except:
        pass
    try:
        _CREATE_TEXT_FILE_WITH_CONTENT("/root/user/model/recorded_face_features.py", json.dumps(FACERECGNIZER.features), "\r\n")

    except:
        pass

load saved face data -->
    import json

    try:
        with open("/root/user/model/recorded_face_features.py", "r") as file:
            FACERECGNIZER.features = json.loads(file.read())
    except:
        pass

result from screen -->
    name of face: FACERECGNIZER.recognize(img_face_Recognizer)[0]
    conf: FACERECGNIZER.recognize(img_face_Recognizer)[1]

self-learning ini -->
    from maix import nn

    CLASSNUM = 3
    SAMPLENUM = 15
    CLASSNAMELIST = ["Object 1 Name", "Object 2 Name", "Object 3 Name"]

load to class -->
    from maix import nn
    from maix.nn.app.classifier import Classifier

    class Self_learn:
        model = {
            "param": "/root/preset/model/resnet18_1000_awnn.param",
            "bin": "/root/preset/model/resnet18_1000_awnn.bin"
        }
        options = {
            "model_type":  "awnn",
            "inputs": {
                "input0": (224, 224, 3)
            },
            "outputs": {
                "190": (1, 1, 512)
            },
            "mean": [127.5, 127.5, 127.5],
            "norm": [0.0176, 0.0176, 0.0176],
        }
        class_num = CLASSNUM  #学习类别
        sample_num = SAMPLENUM  #学习类别总数量
        curr_class = 0
        curr_sample = 0
        def __init__(self):
            # print("-- load model:", self.model)
            self.m = nn.load(self.model, opt=self.options)
            # print("-- load ok")
            # print("-- load classifier")
            self.classifier = Classifier(self.m, self.class_num, self.sample_num, 512, 224, 224)
            # print("-- load ok")

    SELFLEARN = Self_learn()

add class from frame -->
    SELFLEARN.classifier.add_class_img(img_self_learning)

take sample data -->
    for i in range(5):
    SELFLEARN.classifier.add_sample_img(img_self_learning)for i in range(5):
    SELFLEARN.classifier.add_sample_img(img_self_learning)

train -->
    SELFLEARN.classifier.train()

save -->
    SELFLEARN.classifier.save("/root/module.bin")

predict ->
    SELFLEARNidx, SELFLEARNdistance = SELFLEARN.classifier.predict(img_self_learning)

result -->
    name: CLASSNAMELIST[SELFLEARNidx]
    conf: 100-SELFLEARNdistance

ini yolov2 -->
    from maix import nn
    from maix.nn import decoder

    class Yolo:
        labels = ["Object 1 Name", "Object 2 Name", "Object 3 Name"]
        anchors = [1.19, 1.98, 2.79, 4.59, 4.53, 8.92, 8.06, 5.29, 10.32, 10.65]
        m = {
            "param": "/root/preset/model/yolov2_int8.param",
            "bin": "/root/preset/model/yolov2_int8.bin"
        }
        options = {
            "model_type":  "awnn",
            "inputs": {
                "input0": (224, 224, 3)
            },
            "outputs": {
                "output0": (7, 7, (1+4+len(labels))*5)
            },
            "mean": [127.5, 127.5, 127.5],
            "norm": [0.0078125, 0.0078125, 0.0078125],
        }
        def __init__(self):
            from maix import nn
            from maix.nn import decoder
            self.model = nn.load(self.m, opt=self.options)
            self.decoder = decoder.Yolo2(len(self.labels), self.anchors, net_in_size=(224, 224), net_out_size=(7, 7))
        def __del__(self):
            del self.model
            del self.decoder
    Yolo = Yolo()

load % detect -->
    out = Yolo.model.forward(img_modelrecognition, quantize=True, layout="hwc")
    boxes, probs = Yolo.decoder.run(out, nms=0.3, threshold=0.5, img_size=(224, 224))
    if len(boxes):
        for boxesi, box in enumerate(boxes):
            boxes[boxesi].append(probs[boxesi])

results -->
    name: Yolo.labels[i[4][0]]
    x,y,w,h: 0,1,2,3
    conf: i[4][1][i[4][0]]

speech ini -->
    import _coco_mfcc
    import time

    mfcc = _coco_mfcc.MFCC(is_reply=False)

    RecordState = 0
    recordFrequency = 0
    recordResultData = None

speech recongize % plot screen -->
    number_take = 2 # 1 ~ 6

    if RecordState == 0:
        mfcc.recording(recordFrequency)
        recordFrequency = recordFrequency + 1
        time.sleep(1)
        RecordState = 1
    if RecordState == 1:
        recordData = mfcc.state()
        if recordData == mfcc._mfcc_result:
            i.draw_string(0,30,"Recorded successfully", scale = 1, color = (0,204,204) , thickness = 1)
            time.sleep(1)
            if recordFrequency < number_take:
                RecordState = 0
        else:
            i.draw_string(0,30, "Speek  "+str(recordFrequency)+" Speech fragment", scale = 1, color = (0,204,204) , thickness = 1)

start recognize -->
    if RecordState == 0:
        mfcc.recognize()
        recordResultData = None
        time.sleep(1)
        RecordState = 1

when speech recognition has result -->
    if RecordState == 1:
        recordData = mfcc.state()
        if recordData == mfcc._mfcc_result:
            recordResultData = mfcc.result()
            time.sleep(1)
            RecordState = 0

speech snippet -->
    recordResultData == 0 # 0 ~ 5

clean up speech study result -->
    import _coco_mfcc

    mfcc = _coco_mfcc.MFCC(is_reply=False)

    mfcc.clear()
}

{
onl face detect ini -->
    import hashlib
    import base64
    import hmac
    from time import mktime
    from datetime import datetime
    from wsgiref.handlers import format_date_time
    from urllib.parse import urlencode
    import os
    import traceback
    import json
    import requests

    class AssembleHeaderException(Exception):
        def __init__(self, msg):
            self.message = msg

    class Url:
        def __init__(this, host, path, schema):
            this.host = host
            this.path = path
            this.schema = schema
            pass

    # 进行sha256加密和base64编码
    def sha256base64(data):
        sha256 = hashlib.sha256()
        sha256.update(data)
        digest = base64.b64encode(sha256.digest()).decode(encoding="utf-8")
        return digest

    def parse_url_face(requset_url):
        stidx = requset_url.index("://")
        host = requset_url[stidx + 3:]
        schema = requset_url[:stidx + 3]
        edidx = host.index("/")
        if edidx <= 0:
            raise AssembleHeaderException("invalid request url:" + requset_url)
        path = host[edidx:]
        host = host[:edidx]
        u = Url(host, path, schema)
        return u

    def assemble_ws_auth_url_face(requset_url, method="GET", api_key="", api_secret=""):
        u = parse_url_face(requset_url)
        host = u.host
        path = u.path
        now = datetime.now()
        date = format_date_time(mktime(now.timetuple()))
        print(date)
        # date = "Thu, 12 Dec 2019 01:57:27 GMT"
        signature_origin = "host: {}\ndate: {}\n{} {} HTTP/1.1".format(host, date, method, path)
        print(signature_origin)
        signature_sha = hmac.new(api_secret.encode("utf-8"), signature_origin.encode("utf-8"),
                                digestmod=hashlib.sha256).digest()
        signature_sha = base64.b64encode(signature_sha).decode(encoding="utf-8")
        authorization_origin = "api_key=\"%s\", algorithm=\"%s\", headers=\"%s\", signature=\"%s\"" % (
            api_key, "hmac-sha256", "host date request-line", signature_sha)
        authorization = base64.b64encode(authorization_origin.encode("utf-8")).decode(encoding="utf-8")
        print(authorization_origin)
        values = {
            "host": host,
            "date": date,
            "authorization": authorization
        }

        return requset_url + "?" + urlencode(values)

    def gen_body_face(appid, img_path, server_id):
        with open(img_path, "rb") as f:
            img_data = f.read()
        body = {
            "header": {
                "app_id": appid,
                "status": 3
            },
            "parameter": {
                server_id: {
                    "service_kind": "face_detect",
                    #"detect_points": "1", #检测特征点
                    #"detect_property": "1", #检测人脸属性
                    "face_detect_result": {
                        "encoding": "utf8",
                        "compress": "raw",
                        "format": "json"
                    }
                }
            },
            "payload": {
                "input1": {
                    "encoding": "jpg",
                    "status": 3,
                    "image": str(base64.b64encode(img_data), "utf-8")
                }
            }
        }
        return json.dumps(body)

    APPId_face = "0cc32695"
    APISecret_face = "MDAwNGZiZTk0OTYyZjU5NzMzN2VlYWVi"
    APIKey_face = "afa7ae78c88a5fb6e93d836ffa941938"

upload img -->
    ONLINE_IDENT_DATA = ""
    def run_face(appid, apikey, apisecret, img_path, server_id="s67c9c78c"):
        url = "http://api.xf-yun.com/v1/private/{}".format(server_id)
        request_url = assemble_ws_auth_url_face(url, "POST", apikey, apisecret)
        headers = {"content-type": "application/json", "host": "api.xf-yun.com", "app_id": appid}
        print(request_url)
        response = requests.post(request_url, data=gen_body_face(appid, img_path, server_id), headers=headers)
        resp_data = json.loads(response.content.decode("utf-8"))
        print(resp_data)
        picRes=base64.b64decode(resp_data["payload"]["face_detect_result"]["text"]).decode()
        print(picRes)
        with open("/root/pic0.txt","w") as f:
            f.write(picRes)
        return eval(picRes)

    if __name__ == "__main__":
        img_path_face = "/root/preset/img/cocorobo_logo.jpg"
        ONLINE_IDENT_DATA = run_face(appid=APPId_face,apisecret=APISecret_face,apikey=APIKey_face,img_path=img_path_face,)

face_number -->
    ONLINE_IDENT_DATA["face_num"]

result -->
    x,y,w,h: ONLINE_IDENT_DATA.get('face_'+str(i)).get('x')
    conf: ONLINE_IDENT_DATA.get('face_'+str(i)).get('score')

onl tts ini -->
    import time
    import os
    import json
    import wave
    from urllib.parse import urlencode
    from wsgiref.handlers import format_date_time
    from time import mktime
    from datetime import datetime
    import websocket
    import hashlib
    import base64
    import hmac
    import ssl
    import _thread as thread

    STATUS_FIRST_FRAME = 0  # 第一帧的标识
    STATUS_CONTINUE_FRAME = 1  # 中间帧标识
    STATUS_LAST_FRAME = 2  # 最后一帧的标识
    class Ws_Param_Audio(object):
        # 初始化
        def __init__(self, APPID, APIKey, APISecret, Text):
            self.APPID = APPID
            self.APIKey = APIKey
            self.APISecret = APISecret
            self.Text = Text

            # 公共参数(common)
            self.CommonArgs = {"app_id": self.APPID}
            # 业务参数(business)，更多个性化参数可在官网查看
            self.BusinessArgs = {"aue": "raw", "auf": "audio/L16;rate=16000", "vcn": "xiaoyan", "tte": "utf8"}
            self.Data = {"status": 2, "text": str(base64.b64encode(self.Text.encode("utf-8")), "UTF8")}
            #使用小语种须使用以下方式，此处的unicode指的是 utf16小端的编码方式，即"UTF-16LE"”
            #self.Data = {"status": 2, "text": str(base64.b64encode(self.Text.encode('utf-16')), "UTF8")}

        # 生成url
        def create_url(self):
            url = "wss://tts-api.xfyun.cn/v2/tts"
            # 生成RFC1123格式的时间戳
            now = datetime.now()
            date = format_date_time(mktime(now.timetuple()))

            # 拼接字符串
            signature_origin = "host: " + "ws-api.xfyun.cn" + "\n"
            signature_origin += "date: " + date + "\n"
            signature_origin += "GET " + "/v2/tts " + "HTTP/1.1"
            # 进行hmac-sha256进行加密
            signature_sha = hmac.new(self.APISecret.encode("utf-8"), signature_origin.encode("utf-8"),digestmod=hashlib.sha256).digest()
            signature_sha = base64.b64encode(signature_sha).decode(encoding="utf-8")

            authorization_origin = "api_key=\"%s\", algorithm=\"%s\", headers=\"%s\", signature=\"%s\"" % (self.APIKey, "hmac-sha256", "host date request-line", signature_sha)
            authorization = base64.b64encode(authorization_origin.encode("utf-8")).decode(encoding="utf-8")
            # 将请求的鉴权参数组合为字典
            v = {
                "authorization": authorization,
                "date": date,
                "host": "ws-api.xfyun.cn"
            }
            # 拼接鉴权参数，生成url
            url = url + "?" + urlencode(v)
            # print("date: ",date)
            # print("v: ",v)
            # 此处打印出建立连接时候的url,参考本demo的时候可取消上方打印的注释，比对相同参数时生成的url与自己代码生成的url是否一致
            # print('websocket url :', url)
            return url
    APPId_Audio = "0cc32695"
    APISecret_Audio = "MDAwNGZiZTk0OTYyZjU5NzMzN2VlYWVi"
    APIKey_Audio = "afa7ae78c88a5fb6e93d836ffa941938"

do tts % save -->
    def on_message(ws, message):
        try:
            message =json.loads(message)
            code = message["code"]
            sid = message["sid"]
            audio = message["data"]["audio"]
            audio = base64.b64decode(audio)
            status = message["data"]["status"]
            print(message)
            if status == 2:
                print("ws is closed")
                ws.close()
            if code != 0:
                errMsg = message["message"]
                print("sid:%s call error:%s code is:%s" % (sid, errMsg, code))
            else:
                pcm_path = "/demo.pcm"
                with open(pcm_path, "ab") as f:
                    f.write(audio)
                    print("tts")

                with open(pcm_path, "rb") as pcmfile:
                    pcmdata = pcmfile.read()
                with wave.open(pcm_path + ".wav", "wb") as wavfile:
                    wavfile.setparams((1, 2, 16000, 0, "NONE", "NONE"))
                    wavfile.writeframes(pcmdata)

        except Exception as e:
            print("receive msg,but parse exception:", e)

    # 收到websocket错误的处理
    def on_error(ws, error):
        print("### error:", error)
    # 收到websocket关闭的处理
    def on_close(ws):
        print("### closed ###")

    # 收到websocket连接建立的处理
    def on_open(ws):
        def run(*args):
            global wsParam_Audio
            d = {"common": wsParam_Audio.CommonArgs,"business": wsParam_Audio.BusinessArgs,"data": wsParam_Audio.Data,}
            d = json.dumps(d)
            print("------>开始发送文本数据")
            ws.send(d)
            if os.path.exists("/demo.pcm"):
                os.remove("/demo.pcm")

        thread.start_new_thread(run, ())

    wsParam_Audio = ""

    if __name__ == "__main__":
        #with open('word.txt', 'r',encoding="UTF-8")as f:#txt= f.read()
        txt_Audio="hello world"
        if os.path.exists("/demo.pcm"):
            os.remove("/demo.pcm")
        # 测试时候在此处正确填写相关信息即可运行
        wsParam_Audio = Ws_Param_Audio(APPID=APPId_Audio, APISecret=APISecret_Audio,APIKey=APIKey_Audio,Text=txt_Audio)
        websocket.enableTrace(False)
        wsUrl = wsParam_Audio.create_url()
        ws = websocket.WebSocketApp(wsUrl, on_message=on_message, on_error=on_error, on_close=on_close)
        ws.on_open = on_open
        ws.run_forever(sslopt={"cert_reqs": ssl.CERT_NONE})

onl speech recognition ->
    import time
    import websocket
    from datetime import datetime
    import hashlib
    import base64
    import hmac
    import json
    from urllib.parse import urlencode
    import ssl
    from wsgiref.handlers import format_date_time
    from time import mktime
    import _thread as thread

    STATUS_FIRST_FRAME = 0  # 第一帧的标识
    STATUS_CONTINUE_FRAME = 1  # 中间帧标识
    STATUS_LAST_FRAME = 2  # 最后一帧的标识

    class Ws_Param_voice(object):
        # 初始化
        def __init__(self, APPID, APIKey, APISecret, AudioFile):
            self.APPID_voice = APPID
            self.APIKey_voice = APIKey
            self.APISecret_voice = APISecret
            self.AudioFile_voice = AudioFile

            # 公共参数(common)
            self.CommonArgs = {"app_id": self.APPID_voice}
            # 业务参数(business)，更多个性化参数可在官网查看
            self.BusinessArgs = {"domain": "iat", "language": "zh_cn", "accent": "mandarin", "vinfo":1,"vad_eos":10000}

        # 生成url
        def create_url_voice(self):
            url = "wss://ws-api.xfyun.cn/v2/iat"
            # 生成RFC1123格式的时间戳
            now = datetime.now()
            date = format_date_time(mktime(now.timetuple()))

            # 拼接字符串
            signature_origin = "host: " + "ws-api.xfyun.cn" + "\n"
            signature_origin += "date: " + date + "\n"
            signature_origin += "GET " + "/v2/iat " + "HTTP/1.1"
            # 进行hmac-sha256进行加密
            signature_sha = hmac.new(self.APISecret_voice .encode("utf-8"), signature_origin.encode("utf-8"),
                                        digestmod=hashlib.sha256).digest()
            signature_sha = base64.b64encode(signature_sha).decode(encoding="utf-8")

            authorization_origin = "api_key=\"%s\", algorithm=\"%s\", headers=\"%s\", signature=\"%s\"" % (self.APIKey_voice , "hmac-sha256", "host date request-line", signature_sha)
            authorization = base64.b64encode(authorization_origin.encode("utf-8")).decode(encoding="utf-8")
            # 将请求的鉴权参数组合为字典
            v = {
                "authorization": authorization,
                "date": date,
                "host": "ws-api.xfyun.cn"
            }
                # 拼接鉴权参数，生成url
            url = url + "?" + urlencode(v)
            # print("date: ",date)
            # print("v: ",v)
            # 此处打印出建立连接时候的url,参考本demo的时候可取消上方打印的注释，比对相同参数时生成的url与自己代码生成的url是否一致
            # print('websocket url :', url)
            return url


    # 收到websocket消息的处理
    def on_message_voice(ws, message):
        global asr_result,txt_result
        try:
            code = json.loads(message)["code"]
            sid = json.loads(message)["sid"]
            if code != 0:
                errMsg = json.loads(message)["message"]
                print("sid:%s call error:%s code is:%s" % (sid, errMsg, code))

            else:
                data = json.loads(message)["data"]["result"]["ws"]
                # print(json.loads(message))
                result = ""
                for i in data:
                    for w in i["cw"]:
                        result += w["w"]
                print("sid:%s call success!,data is:%s" % (sid, json.dumps(data, ensure_ascii=False)))
                print("json.dumps(data, ensure_ascii=False)",eval(json.dumps(data, ensure_ascii=False)))
                asr_result.append(eval(json.dumps(data, ensure_ascii=False)))
                txt_result.append(eval(json.dumps(data, ensure_ascii=False)[1]))
        except Exception as e:
            print("receive msg,but parse exception:", e)



    # 收到websocket错误的处理
    def on_error_voice(ws, error):
        print("### error:", error)


    # 收到websocket关闭的处理
    def on_close_voice(ws):
        print("### closed ###")


    # 收到websocket连接建立的处理
    def on_open_voice(ws):
        def run_voice(*args):
            global wsParam_voice
            frameSize = 8000  # 每一帧的音频大小
            intervel = 0.04  # 发送音频间隔(单位:s)
            status = STATUS_FIRST_FRAME  # 音频的状态信息，标识音频是第一帧，还是中间帧、最后一帧

            with open(wsParam_voice.AudioFile_voice, "rb") as fp:
                while True:
                    buf = fp.read(frameSize)
                    # 文件结束
                    if not buf:
                        status = STATUS_LAST_FRAME
                    # 第一帧处理
                    # 发送第一帧音频，带business 参数
                    # appid 必须带上，只需第一帧发送
                    if status == STATUS_FIRST_FRAME:

                        d = {"common": wsParam_voice.CommonArgs,
                                "business": wsParam_voice.BusinessArgs,
                                "data": {"status": 0, "format": "audio/L16;rate=16000",
                                        "audio": str(base64.b64encode(buf), "utf-8"),
                                        "encoding": "raw"}}
                        d = json.dumps(d)
                        ws.send(d)
                        status = STATUS_CONTINUE_FRAME
                    # 中间帧处理
                    elif status == STATUS_CONTINUE_FRAME:
                        d = {"data": {"status": 1, "format": "audio/L16;rate=16000",
                                        "audio": str(base64.b64encode(buf), "utf-8"),
                                        "encoding": "raw"}}
                        ws.send(json.dumps(d))
                    # 最后一帧处理
                    elif status == STATUS_LAST_FRAME:
                        d = {"data": {"status": 2, "format": "audio/L16;rate=16000",
                                        "audio": str(base64.b64encode(buf), "utf-8"),
                                        "encoding": "raw"}}
                        ws.send(json.dumps(d))
                        time.sleep(1)
                        break
                    # 模拟音频采样间隔
                    time.sleep(intervel)
            ws.close()

        thread.start_new_thread(run_voice, ())

    asr_result=[]
    txt_result=[]
    APPId_voice = "0cc32695"
    APISecret_voice = "MDAwNGZiZTk0OTYyZjU5NzMzN2VlYWVi"
    APIKey_voice = "afa7ae78c88a5fb6e93d836ffa941938"

load from path -->
    asr_string=""
    def getVoiceData():
        global wsParam_voice,asr_string,txt_result,asr_result
        # 测试时候在此处正确填写相关信息即可运行
        asr_string = ""
        txt_result = []
        asr_result = []
        time1 = datetime.now()
        wsParam_voice = Ws_Param_voice(APPID=APPId_voice, APISecret=APISecret_voice,
                                                APIKey=APIKey_voice,
                                                AudioFile="/root/user/audio/record.wav")
        websocket.enableTrace(False)
        wsUrl = wsParam_voice.create_url_voice()
        ws = websocket.WebSocketApp(wsUrl, on_message=on_message_voice, on_error=on_error_voice)
        ws.on_open = on_open_voice
        ws.run_forever(sslopt={"cert_reqs": ssl.CERT_NONE})
        time2 = datetime.now()

        for i in range(len(asr_result)):
            for j in range(len(asr_result[i])):
                txt_result.append(asr_result[i][j]["cw"][0]["w"])
                print("txt_result:", txt_result)
        for i in range(len(txt_result)):
            asr_string=asr_string+txt_result[i]
            print("asr_string:", asr_string)
        return asr_string

    VoiceResult = getVoiceData()

result -->
    x = VoiceResult

onl license recognition -->
    from urllib.parse import urlencode
    from wsgiref.handlers import format_date_time
    from time import mktime
    from datetime import datetime
    import hashlib
    import base64
    import hmac
    import traceback
    import json
    import requests

    APPId_Card = "0cc32695"
    APISecret_Card = "MDAwNGZiZTk0OTYyZjU5NzMzN2VlYWVi"
    APIKey_Card = "afa7ae78c88a5fb6e93d836ffa941938"

    class AssembleHeaderException_Card(Exception):
        def __init__(self, msg):
            self.message = msg

    class Url_Card:
        def __init__(this, host, path, schema):
            this.host = host
            this.path = path
            this.schema = schema
            pass

    # 进行sha256加密和base64编码
    def sha256base64_Card(data):
        sha256 = hashlib.sha256()
        sha256.update(data)
        digest = base64.b64encode(sha256.digest()).decode(encoding="utf-8")
        return digest

    def parse_url_Card(requset_url):
        stidx = requset_url.index("://")
        host = requset_url[stidx + 3:]
        schema = requset_url[:stidx + 3]
        edidx = host.index("/")
        if edidx <= 0:
            raise AssembleHeaderException_Card("invalid request url:" + requset_url)
        path = host[edidx:]
        host = host[:edidx]
        u = Url_Card(host, path, schema)
        return u

    def assemble_ws_auth_url_Card(requset_url, method="GET", api_key="", api_secret=""):
        u = parse_url_Card(requset_url)
        host = u.host
        path = u.path
        now = datetime.now()
        date = format_date_time(mktime(now.timetuple()))
        print(date)
        # date = "Thu, 12 Dec 2019 01:57:27 GMT"
        signature_origin = "host: {}\ndate: {}\n{} {} HTTP/1.1".format(host, date, method, path)
        print(signature_origin)
        signature_sha = hmac.new(api_secret.encode("utf-8"), signature_origin.encode("utf-8"),
                                    digestmod=hashlib.sha256).digest()
        signature_sha = base64.b64encode(signature_sha).decode(encoding="utf-8")
        authorization_origin = "api_key=\"%s\", algorithm=\"%s\", headers=\"%s\", signature=\"%s\"" % (
                api_key, "hmac-sha256", "host date request-line", signature_sha)
        authorization = base64.b64encode(authorization_origin.encode("utf-8")).decode(encoding="utf-8")
        print(authorization_origin)
        values = {
            "host": host,
            "date": date,
            "authorization": authorization
        }

        return requset_url + "?" + urlencode(values)

    def gen_body_Card(appid, img_path, server_id):
        with open(img_path, "rb") as f:
            img_data = f.read()
        body = {
            "header": {
                "app_id": appid,
                "status": 3
            },
            "parameter": {
                server_id: {
                    "carLicenseRes": {
                        "encoding": "utf8",
                        "compress": "raw",
                        "format": "json"
                    }
                }
            },
            "payload": {
                "carImgBase64Str": {
                    "encoding": "jpg",
                    "status": 3,
                    "image": str(base64.b64encode(img_data), "utf-8")
                }
            }
        }
        return json.dumps(body)

load % license detect -->
    def run_Card(appid, apikey, apisecret, img_path, server_id="jd_ocr_car"):
        url = "http://api.xf-yun.com/v1/private/{}".format(server_id)
        request_url = assemble_ws_auth_url_Card(url, "POST", apikey, apisecret)
        headers = {"content-type": "application/json", "host": "api.xf-yun.com", "app_id": appid}
        #print(request_url)
        response = requests.post(request_url, data=gen_body_Card(appid, img_path, server_id), headers=headers)
        resp_data = json.loads(response.content.decode("utf-8"))
        print(resp_data)
        resultCard = base64.b64decode(resp_data["payload"]["carLicenseRes"]["text"]).decode("utf-8")
        return json.loads(resultCard)

    CARDRESULT = ""



    if __name__ == "__main__":
        img_path_Card = "/root/preset/img/cocorobo_logo.jpg"
        CARDRESULT = run_Card(appid=APPId_Card,apisecret=APISecret_Card,apikey=APIKey_Card,img_path=img_path_Card,)

result -->
    CARDRESULT

onl translate (chi <-> eng) -->
    from datetime import datetime
    import requests
    import hashlib
    import base64
    import hmac
    import json


    APPId_machine = "0cc32695"
    APISecret_machine = "MDAwNGZiZTk0OTYyZjU5NzMzN2VlYWVi"
    APIKey_machine = "afa7ae78c88a5fb6e93d836ffa941938"
    TRANSLATIONTEXT = ""
    class get_result_machine(object):
        def __init__(self,host,TRANSLATIONTEXT):
            # 应用ID（到控制台获取）
            self.APPID_machine = "0cc32695"
            # 接口APISercet（到控制台机器翻译服务页面获取）
            self.Secret_machine = "MDAwNGZiZTk0OTYyZjU5NzMzN2VlYWVi"
            # 接口APIKey（到控制台机器翻译服务页面获取）
            self.APIKey_machine= "afa7ae78c88a5fb6e93d836ffa941938"


            # 以下为POST请求
            self.Host = "itrans.xfyun.cn"
            self.RequestUri = "/v2/its"
            # 设置url
            # print(host)
            self.url="https://"+host+self.RequestUri
            self.HttpMethod = "POST"
            self.Algorithm = "hmac-sha256"
            self.HttpProto = "HTTP/1.1"

            # 设置当前时间
            curTime_utc = datetime.utcnow()
            self.Date = self.httpdate_machine(curTime_utc)
            # 设置业务参数
            # 语种列表参数值请参照接口文档：https://www.xfyun.cn/doc/nlp/xftrans/API.html
            self.Text=TRANSLATIONTEXT
            self.BusinessArgs={
                    "from": "cn",
                    "to": "en",
                }

        def hashlib_256_machine(self, res):
            m = hashlib.sha256(bytes(res.encode(encoding="utf-8"))).digest()
            result = "SHA-256=" + base64.b64encode(m).decode(encoding="utf-8")
            return result

        def httpdate_machine(self, dt):
            """
            Return a string representation of a date according to RFC 1123
            (HTTP/1.1).

            The supplied date must be in UTC.

            """
            weekday = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"][dt.weekday()]
            month = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep",
                    "Oct", "Nov", "Dec"][dt.month - 1]
            return "%s, %02d %s %04d %02d:%02d:%02d GMT" % (weekday, dt.day, month,
                                                            dt.year, dt.hour, dt.minute, dt.second)

        def generateSignature_machine(self, digest):
            signatureStr = "host: " + self.Host + "\n"
            signatureStr += "date: " + self.Date + "\n"
            signatureStr += self.HttpMethod + " " + self.RequestUri \
                            + " " + self.HttpProto + "\n"
            signatureStr += "digest: " + digest
            signature = hmac.new(bytes(self.Secret_machine.encode(encoding="utf-8")),
                                bytes(signatureStr.encode(encoding="utf-8")),
                                digestmod=hashlib.sha256).digest()
            result = base64.b64encode(signature)
            return result.decode(encoding="utf-8")

        def init_header_machine(self, data):
            digest = self.hashlib_256_machine(data)
            #print(digest)
            sign = self.generateSignature_machine(digest)
            authHeader = "api_key=\""+self.APIKey_machine+"\", algorithm=\""+self.Algorithm+"\", headers=\"host date request-line digest\", signature=\""+sign+"\""
            #print(authHeader)
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json",
                "Method": "POST",
                "Host": self.Host,
                "Date": self.Date,
                "Digest": digest,
                "Authorization": authHeader
            }
            return headers

        def get_body_machine(self):
            content = str(base64.b64encode(self.Text.encode("utf-8")), "utf-8")
            postdata = {
                "common": {"app_id": self.APPID_machine},
                "business": self.BusinessArgs,
                "data": {
                    "text": content,
                }
            }
            body = json.dumps(postdata)
            #print(body)
            return body

        def call_url_machine(self):
            if self.APPID_machine == "" or self.APIKey_machine == "" or self.Secret_machine == "":
                print("Appid 或APIKey 或APISecret 为空！请打开demo代码，填写相关信息。")
            else:
                code = 0
                body=self.get_body_machine()
                headers=self.init_header_machine(body)
                #print(self.url)
                response = requests.post(self.url, data=body, headers=headers,timeout=8)
                status_code = response.status_code
                #print(response.content)
                if status_code!=200:
                    # 鉴权失败
                    print("Http请求失败，状态码：" + str(status_code) + "，错误信息：" + response.text)
                    print("请根据错误信息检查代码，接口文档：https://www.xfyun.cn/doc/nlp/xftrans/API.html")
                else:
                    # 鉴权成功
                    respData = json.loads(response.text)
                    #print('result',respData["data"]['result']['trans_result']['dst'])
                    # return
                    # 以下仅用于调试
                    code = str(respData["code"])
                    if code=="0":
                        return respData["data"]["result"]["trans_result"]["dst"]
                    else:
                        print("请前往https://www.xfyun.cn/document/error-code?code=" + code + "查询解决办法")

translate -->
    TRANSLATIONTEXT = "hello world"
    machinehost = "itrans.xfyun.cn"
    #初始化类
    gClass=get_result_machine(host=machinehost,TRANSLATIONTEXT=TRANSLATIONTEXT)
    TRANSLATIONRESULT = gClass.call_url_machine()

result -->
    TRANSLATIONRESULT

onl handwrite txt recognition ini -->
    import time
    import base64
    import hashlib
    import requests
    import json
    from urllib import parse

    URL_handwritten_text = "http://webapi.xfyun.cn/v1/service/v1/ocr/handwriting"
    APPId_handwritten_text = "0cc32695"
    APIKey_handwritten_text = "afa7ae78c88a5fb6e93d836ffa941938"
    def textGetHeader_handwritten_text(language, location):
        curTime = str(int(time.time()))
        param = "{\"language\":\""+language+"\",\"location\":\""+location+"\"}"
        paramBase64 = base64.b64encode(param.encode("utf-8"))

        m2 = hashlib.md5()
        str1 = APIKey_handwritten_text + curTime + str(paramBase64, "utf-8")
        m2.update(str1.encode("utf-8"))
        checkSum = m2.hexdigest()
        # 组装http请求头
        header = {
            "X-CurTime": curTime,
            "X-Param": paramBase64,
            "X-Appid": APPId_handwritten_text,
            "X-CheckSum": checkSum,
            "Content-Type": "application/x-www-form-urlencoded; charset=utf-8",
        }
        return header
    def getBody_handwritten_text(filepath):
        with open(filepath, "rb") as f:
            imgfile = f.read()
        data = {"image": str(base64.b64encode(imgfile), "utf-8")}
        return data

load % handwrite detect -->
    # 语种设置
    textLanguage = "cn|en"
    # 是否返回文本位置信息
    textLocation = "false"
    # 图片上传接口地址
    picFilePath_handwritten_text = "/root/preset/img/cocorobo_logo.jpg"
    HandwrittenTextData = requests.post(URL_handwritten_text, headers=textGetHeader_handwritten_text(textLanguage, textLocation), data=getBody_handwritten_text(picFilePath_handwritten_text))

result -->
    def handwrittenTextDatas_handwritten_text(r):
        my_json_data = r.content.decode("utf8")
        # print(type(my_json))
        loadsData = json.loads(my_json_data)
        # s = json.dumps(data, indent=4, sort_keys=True)
        print("data",my_json_data)
        deal_data = ""
        if loadsData["code"] == "0" :
            t = loadsData["data"]["block"][0]["line"]
            for i in t:
                print(i)
                for j in i["word"]:
                    print(j["content"])
                    deal_data = deal_data + j["content"] + " "

        #print('deal_data',deal_data)
        return deal_data

    handwrittenTextDatas_handwritten_text(HandwrittenTextData)

onl formula recognition -->
    import time
    import base64
    import hashlib
    import requests
    import hmac
    import json
    from urllib import parse
    from datetime import datetime

    class get_result_formula(object):
        def __init__(self,host,path):
            # 应用ID（到控制台获取）
            self.APPID_formula = "0cc32695"
            # 接口APISercet（到控制台公式识别服务页面获取）
            self.Secret_formula = "MDAwNGZiZTk0OTYyZjU5NzMzN2VlYWVi"
            # 接口APIKey（到控制台公式识别服务页面获取）
            self.APIKey_formula = "afa7ae78c88a5fb6e93d836ffa941938"


            # 以下为POST请求
            self.Host_formula = host
            self.RequestUri_formula = "/v2/itr"
            # 设置url
            # print(host)
            self.url_formula="https://"+host+self.RequestUri_formula
            self.HttpMethod_formula = "POST"
            self.Algorithm_formula = "hmac-sha256"
            self.HttpProto_formula = "HTTP/1.1"

            # 设置当前时间
            curTime_utc = datetime.utcnow()
            self.Date_formula = self.httpdate_formula(curTime_utc)
            #设置测试图片文件
            self.AudioPath_formula = path
            self.BusinessArgs_formula={
                    "ent": "teach-photo-print",
                    "aue": "raw",
                }

        def imgRead_formula(self, path):
            with open(path, "rb") as fo:
                return fo.read()

        def hashlib_256_formula(self, res):
            m = hashlib.sha256(bytes(res.encode(encoding="utf-8"))).digest()
            result = "SHA-256=" + base64.b64encode(m).decode(encoding="utf-8")
            return result

        def httpdate_formula(self, dt):
            """
            Return a string representation of a date according to RFC 1123
            (HTTP/1.1).

            The supplied date must be in UTC.

            """
            weekday = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"][dt.weekday()]
            month = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep",
                    "Oct", "Nov", "Dec"][dt.month - 1]
            return "%s, %02d %s %04d %02d:%02d:%02d GMT" % (weekday, dt.day, month,
                                                            dt.year, dt.hour, dt.minute, dt.second)

        def generateSignature_formula(self, digest):
            signatureStr = "host: " + self.Host_formula + "\n"
            signatureStr += "date: " + self.Date_formula + "\n"
            signatureStr += self.HttpMethod_formula + " " + self.RequestUri_formula \
                            + " " + self.HttpProto_formula + "\n"
            signatureStr += "digest: " + digest
            signature = hmac.new(bytes(self.Secret_formula.encode(encoding="utf-8")),
                                bytes(signatureStr.encode(encoding="utf-8")),
                                digestmod=hashlib.sha256).digest()
            result = base64.b64encode(signature)
            return result.decode(encoding="utf-8")

        def init_header_formula(self, data):
            digest = self.hashlib_256_formula(data)
            #print(digest)
            sign = self.generateSignature_formula(digest)
            authHeader = "api_key=\""+self.APIKey_formula+"\", algorithm=\""+self.Algorithm_formula+"\", headers=\"host date request-line digest\", signature=\""+sign+"\""
            #print(authHeader)
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json",
                "Method": "POST",
                "Host": self.Host_formula,
                "Date": self.Date_formula,
                "Digest": digest,
                "Authorization": authHeader
            }
            return headers

        def get_body_formula(self):
            audioData = self.imgRead_formula(self.AudioPath_formula)
            content = base64.b64encode(audioData).decode(encoding="utf-8")
            postdata = {
                "common": {"app_id": self.APPID_formula},
                "business": self.BusinessArgs_formula,
                "data": {
                    "image": content,
                }
            }
            body = json.dumps(postdata)
            #print(body)
            return body

        def call_url_formula(self):
            if self.APPID_formula == "" or self.APIKey_formula == "" or self.Secret_formula == "":
                print("Appid 或APIKey 或APISecret 为空！请打开demo代码，填写相关信息。")
            else:
                code = 0
                body=self.get_body_formula()
                headers=self.init_header_formula(body)
                #print(self.url_formula)
                response = requests.post(self.url_formula, data=body, headers=headers,timeout=8)
                status_code = response.status_code
                #print(response.content)
                if status_code!=200:
                    # 鉴权失败
                    print("Http请求失败，状态码：" + str(status_code) + "，错误信息：" + response.text)
                    print("请根据错误信息检查代码，接口文档：https://www.xfyun.cn/doc/words/formula-discern/API.html")
                else:
                    # 鉴权成功
                    respData = json.loads(response.text)
                    dataCotent = ""
                    try:
                        for i in respData["data"]["region"]:
                            dataCotent += i["recog"]["content"]
                    except:
                        dataCotent = "Failure"
                    return dataCotent

    FORMULARESULT = ""

load % formula detection -->
    if __name__ == "__main__":
        formulaUrlPath_formula = "/root/preset/img/cocorobo_logo.jpg"
        ##示例:  host="rest-api.xfyun.cn"域名形式
        host = "rest-api.xfyun.cn"
        #初始化类
        gClass=get_result_formula(host,path=formulaUrlPath_formula)
        FORMULARESULT = gClass.call_url_formula()

result -->
    FORMULARESULT

onl emotion detection ini -->
    import base64
    import requests
    import json

    def get_img_base64str(single_image_path):
        with open(single_image_path, "rb") as fp:
            imgbase64 = base64.b64encode(fp.read())
            return imgbase64.decode()

    url = "https://ai-api.cocorobo.hk/face"
    EMOTION_SEND_REQUEST = {}

load % emotion detect -->
    imageEmotionURLbase64 = str(get_img_base64str("/root/preset/img/cocorobo_logo.jpg"))
        # print(imageURLbase64)
    data = "{\"image\":\"" + str(imageEmotionURLbase64) + "\"}"
    try:
        EMOTION_SEND_REQUEST = requests.post(url, data = data , headers = { "Ocp-Apim-Subscription-Key": "6baf787532ef4eec8bc93c88518b8916", "Content-type": "application/json" }, timeout = 10000)
        EMOTION_SEND_REQUEST = json.loads(EMOTION_SEND_REQUEST.content)["data"]["result"]["face_list"][0]
        # print(str(_COCOCLOUD_SEND_REQUEST.status_code)+", "+str(_COCOCLOUD_SEND_REQUEST.content))
    except BaseException as e:
        EMOTION_SEND_REQUEST = "no result"
    pass

result -->
    age: EMOTION_SEND_REQUEST["age"]
    emotion: EMOTION_SEND_REQUEST["emotion"]["type"]
    gender: EMOTION_SEND_REQUEST["gender"]["type"]
    expression: EMOTION_SEND_REQUEST["expression"]["type"]

onl geseture recognition ini -->
    from PIL import Image
    import base64
    import requests
    import json

    def getImgData(single_image_path):
        imgGesture = Image.open(single_image_path).convert("RGB")
        imgGesture.save(single_image_path.split(".")[0] + ".png", "png")
        with open(single_image_path.split(".")[0] + ".png", "rb") as fp:
            return fp.read()

    url = "https://ai-api.cocorobo.hk/gesture"

load % gesture detect -->
    try:
        payload={}
        files=[("image",("filename.png", getImgData("/root/preset/img/cocorobo_logo.jpg"),"image/png"))]
        headers = {}
        GESTURE_SEND_REQUEST = requests.post(url, headers=headers, data=payload, files=files, timeout = 10000)
        GESTURE_SEND_REQUEST = json.loads(GESTURE_SEND_REQUEST.content)["data"]["result"][0]["classname"]
        # print(str(_COCOCLOUD_SEND_REQUEST.status_code)+", "+str(_COCOCLOUD_SEND_REQUEST.content))
    except BaseException as e:
        GESTURE_SEND_REQUEST = "no result"
    pass

result -->
    GESTURE_SEND_REQUEST
}

{
wifi ini -->
    import os

    def getNetworkDate_noexit():
        global getDateNum
        try:
            coon = http.client.HTTPConnection("www.baidu.com")
            coon.request("GET","/")
            r = coon.getresponse()
            ts = r.getheader("date")
            GMT_time = time.strptime(ts[5:25],"%d %b %Y %H:%M:%S")
            BeiJing_time = time.localtime(time.mktime(GMT_time) + 8*60*60)
            format_time = time.strftime("%Y-%m-%d %H:%M:%S",BeiJing_time)
            command = "date -s "+"\"{}\"".format(format_time)
            os.system(command)
            getDateNum = 1
            # sys.exit()
        except:
            pass

    os.system("wifi_disconnect_ap_test")
    os.system("wifi_connect_chinese_ap_test ""ENTER_YOUR_SSID"" ""ENTER_YOUR_PASSWORD""")
    #CLIENT = ntplib.NTPClient()
    #RESPONSE = CLIENT.request('127.0.0.1')
    getNetworkDate_noexit()

wifi state -->
    import os
    import sys
    sys.path.append("/root/")
    import time
    import http.client

    def wifi_is_content():
        global getDateNum
        cmd = "ifconfig"

        res = os.popen(cmd).read()
        data = False
        if res.rfind("inet addr:")!=48:
            data = True

        return data

    wifi_is_content()

disconnect -->
    import os

    os.system("wifi_disconnect_ap_test")

sync time -->
    import os
    import sys
    sys.path.append("/root/")
    import time
    import http.client

    def getNetworkDate():
        try:
            coon = http.client.HTTPConnection("www.baidu.com")
            coon.request("GET","/")
            r = coon.getresponse()
            ts = r.getheader("date")
            GMT_time = time.strptime(ts[5:25],"%d %b %Y %H:%M:%S")
            BeiJing_time = time.localtime(time.mktime(GMT_time)+ 8*60*60)
            format_time = time.strftime("%Y-%m-%d %H:%M:%S",BeiJing_time)
            command = "date -s "+"\"{}\"".format(format_time)
            os.system(command)
            sys.exit()
        except:
            pass

    getNetworkDate()
}

{
weather ini -->
    import json
    import requests
    import bs4

    def get_html(url):
        headers = {
            "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36",
            "ContentType":
            "text/html; charset=utf-8",
            "Accept-Encoding":
            "gzip, deflate, sdch",
            "Accept-Language":
            "zh-CN,zh;q=0.8",
            "Connection":
            "keep-alive",
        }
        try:
            htmlcontet = requests.get(url, headers=headers, timeout=30)
            htmlcontet.raise_for_status()
            htmlcontet.encoding = "utf-8"
            return htmlcontet.text
        except:
            return " 请求失败 "

    def get_content(url):
        weather_list = []
        html = get_html(url)
        soup = bs4.BeautifulSoup(html, "lxml")
        content_ul = soup.find("div", class_="t").find("ul", class_="clearfix").find_all("li")

        for content in content_ul:
            try:
                weather = {}
                weather["D"] = content.find("h1").text
                weather["WEA"] = content.find("p", class_="wea").text
                weather["T"] = content.find(
                    "p", class_="tem").span.text + content.find(
                        "p", class_="tem").em.text
                weather_list.append(weather)
            except:
                print("查询不到")
        print(weather_list)
        return weather_list

result -->
    BeiJing: get_content("http://www.weather.com.cn/weather1d/101010100.shtml")
    ShangHai: get_content("http://www.weather.com.cn/weather1d/101020100.shtml")
    GuangZhou: get_content("http://www.weather.com.cn/weather1d/101280101.shtml")
    ShenZhen: get_content("http://www.weather.com.cn/weather1d/101280601.shtml")
    HongKong: get_content("http://www.weather.com.cn/weather1d/101320101.shtml")

IFTTT update value -->
    import requests
    import json

    _IFTTT_POST_API_KEY = "ENTER_YOUR_API_KEY"
    _IFTTT_POST_EVENT_NAME = "ENTER_YOUR_EVENT_NAME"
    _IFTTT_POST_ENDPOINT = "http://maker.ifttt.com/trigger/"+ _IFTTT_POST_EVENT_NAME + "/with/key/" + _IFTTT_POST_API_KEY
    _IFTTT_POST_DATA = '{"value1":"'+ str(0) +'","value2":"'+ str(0) +'","value3":"'+ str(0) +'"}'
    _IFTTT_POST_REQUEST = requests.post(_IFTTT_POST_ENDPOINT, data = _IFTTT_POST_DATA , headers = { "Content-type": "application/json" }, timeout=60)

IFTTT post url -->
    import requests
    import json

    _IFTTT_TRIGGER_EVENT_NAME = "ENTER_YOUR_EVENT_NAME"
    _IFTTT_TRIGGER_API_KEY = "ENTER_YOUR_API_KEY"
    _IFTTT_TRIGGER_ENDPOINT = "https://maker.ifttt.com/trigger/" + _IFTTT_TRIGGER_EVENT_NAME + "/with/key/" + _IFTTT_TRIGGER_API_KEY

    _IFTTT_GET_REQUEST = requests.get(_IFTTT_TRIGGER_ENDPOINT, timeout=60)
}

{
cococloud update value (int) -->
    import requests

    _COCOCLOUD_SEND_REQUEST = None

    _COCOCLOUD_SEND_ENDPOINT = "http://api.cocorobo.hk/iot/data/eventAPIKeyJson/ENTER_YOUR_EVENT_API_KEY"
    _COCOCLOUD_SEND_DATA = {"Property0":0}
    try:
        _COCOCLOUD_SEND_REQUEST = requests.post(_COCOCLOUD_SEND_ENDPOINT, json = _COCOCLOUD_SEND_DATA , headers = { "Content-type": "application/json" }, timeout = 60)
        print(str(_COCOCLOUD_SEND_REQUEST.status_code)+", "+str(_COCOCLOUD_SEND_REQUEST.content))
    except BaseException as e:
        print(e)
    pass

cococloud update value (str) -->
    import requests

    _COCOCLOUD_SEND_REQUEST = None

    _COCOCLOUD_SEND_ENDPOINT = "http://api.cocorobo.hk/iot/data/eventAPIKeyJson/ENTER_YOUR_EVENT_API_KEY"
    _COCOCLOUD_SEND_DATA = {"Property":"value"}
    try:
        _COCOCLOUD_SEND_REQUEST = requests.post(_COCOCLOUD_SEND_ENDPOINT, json = _COCOCLOUD_SEND_DATA , headers = { "Content-type": "application/json" }, timeout = 60)
        print(str(_COCOCLOUD_SEND_REQUEST.status_code)+", "+str(_COCOCLOUD_SEND_REQUEST.content))
    except BaseException as e:
        print(e)
    pass

cocorobo post url -->
    import requests

    _COCOCLOUD_READ_REQUEST = None

    _COCOCLOUD_READ_REQUEST = requests.get("http://api.cocorobo.hk/iot/data/eventAPIKeyJson/" + "ENTER_YOUR_EVENT_API_KEY", timeout=60)

cocorobo get value (str) -->
    _COCOCLOUD_READ_REQUEST.json()['data'][0]["property"]
}
